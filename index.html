<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta charset="UTF-8"> <!-- Simplified charset declaration -->
    <title>Sheng Cheng</title>
    <meta name="author" content="Sheng Cheng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" href="stylesheet.css">
    <!-- Removed inline styles and table-based layout -->
  </head>

  <body>
    <!-- Container for the entire content -->
    <div class="container">
      
      <!-- Header Section -->
      <header class="header">
        <p class="name">Sheng Cheng</p>
        <p class="intro">
          I'm a PhD student at ASU. I'm working with 
          <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>, and co-advised by 
          <a href="http://public.asu.edu/~yren32">Yi Ren</a>. I closely collaborate with 
          <a href="https://maitreyapatel.com/">Maitreya Patel</a>, 
          <a href="https://www.changhoonkim.com/">Changhoon Kim</a>, 
          <a href="https://sites.google.com/view/deqiankong/home">Deqian Kong</a>. 
          Previously, I received my M.Eng. in Electrical Engineering from the University of Illinois at Urbana-Champaign, working with 
          <a href="https://ruoyus.github.io/">Ruoyu Sun</a>, and received B.S. from Huazhong University of Science and Technology.
        </p>
        <p class="links">
          <a href="mailto:scheng53@asu.edu">Email</a> &nbsp;/&nbsp;
          <a href="https://scholar.google.com/citations?user=TWAwdYsAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
          <a href="https://twitter.com/chengshengcs">Twitter</a> &nbsp;/&nbsp;
          <a href="https://github.com/shengcheng/">Github</a> &nbsp;/&nbsp;
          <a href="https://www.linkedin.com/in/sheng-cheng-661826118/">Linkedin</a>
        </p>
      </header>

      <!-- Research Section -->
      <section class="section">
        <h2>Research</h2>
        <p>
          I'm interested in computer vision and machine learning. My focus lies in the areas of vision & language (particularly in Text-to-Image generation), domain generalization & robustness, and AI in Science.
        </p>
      </section>

      <!-- Publications Section -->
      <section class="section publications">
        <h2>Publications</h2>

        <!-- Publication Item 1 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/triplet.png" alt="TripletCLIP">
          </div>
          <div class="publication-content">
            <a href="https://tripletclip.github.io/" class="papertitle">TripletCLIP: Improving Compositional Reasoning of CLIP via Vision-Language Negatives</a>
            <p>
              <a href="https://maitreyapatel.com/">Maitreya Patel</a>, 
              <a href="https://www.linkedin.com/in/abhiram-kusumba-a077b8174/">Abhiram Kusumba</a>, 
              <strong>Sheng Cheng</strong>, 
              <a href="https://www.changhoonkim.com/">Changhoon Kim</a>, 
              <a href="https://www.tejasgokhale.com/">Tejas Gokhale</a>, 
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>, 
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
            </p>
            <p><em>NeurIPS 2024</em></p>
            <p>
              <a href="https://tripletclip.github.io/">Project Page</a> / 
              <a href="https://arxiv.org/abs/2411.02545">arXiv</a> / 
              <a href="https://github.com/tripletclip/TripletCLIP">Code</a>
            </p>
            <p>
              We enhance CLIP models by generating "hard" negative captions and images to improve their compositional reasoning ability.
            </p>
          </div>
        </div>

        <!-- Publication Item 2 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/score.png" alt="Precision or Recall?">
          </div>
          <div class="publication-content">
            <a href="https://aclanthology.org/2024.findings-emnlp.211/" class="papertitle">Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model</a>
            <p>
              <strong>Sheng Cheng</strong>, 
              <a href="https://maitreyapatel.com/">Maitreya Patel</a>, 
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
            </p>
            <p><em>EMNLP 2024, Findings</em></p>
            <p>
              <a href="https://arxiv.org/abs/2411.05079">arXiv</a> / 
              <a href="https://github.com/shengcheng/Captions4T2I">Code</a>
            </p>
            <p>
              We analyze the impact of precision and recall in human-annotated and synthetic captions on the training text-to-image models.
            </p>
          </div>
        </div>

        <!-- Publication Item 3 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/ode.png" alt="Latent Space Energy-based Neural ODEs">
          </div>
          <div class="publication-content">
            <a href="https://www.arxiv.org/abs/2409.03845" class="papertitle">Latent Space Energy-based Neural ODEs</a>
            <p>
              <strong>Sheng Cheng*</strong>, 
              <a href="https://sites.google.com/view/deqiankong/home">Deqian Kong*</a>, 
              <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>, 
              <a href="https://klee44.github.io/">Kookjin Lee</a>, 
              <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu‡</a>, 
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang‡</a>
            </p>
            <p><em>Preprint, 2024</em></p>
            <p>
              <a href="https://www.arxiv.org/abs/2409.03845">arXiv</a>
            </p>
            <p>
              Integrating energy-based prior model with Neural ODEs for latent space continuous-time sequence data modeling, training using MLE with MCMC instead of inference network.
            </p>
          </div>
        </div>

        <!-- Publication Item 4 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/eclipse_teaser.png" alt="Revising Text-to-Image Prior">
          </div>
          <div class="publication-content">
            <a href="https://eclipse-t2i.vercel.app/" class="papertitle">Revising Text-to-Image Prior for Improved Text Conditioned Image Generations</a>
            <p>
              <a href="https://maitreyapatel.com/">Maitreya Patel</a>, 
              <a href="https://www.changhoonkim.com/">Changhoon Kim</a>, 
              <strong>Sheng Cheng</strong>, 
              <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>, 
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
            </p>
            <p><em>CVPR 2024</em></p>
            <p>
              <a href="https://eclipse-t2i.vercel.app/">Project Page</a> / 
              <a href="https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2">Demo</a> / 
              <a href="https://arxiv.org/abs/2312.04655">arXiv</a> / 
              <a href="https://github.com/eclipse-t2i/eclipse-inference">Code</a> / 
              Media coverage (
              <a href="https://x.com/_akhaliq/status/1734036192817971630?s=61&t=3KlNMV5Ioc9Fy-B0z6w57A">Twitter of AK</a>, 
              <a href="https://www.marktechpost.com/2023/12/13/this-ai-research-from-arizona-state-university-unveil-eclipse-a-novel-contrastive-learning-strategy-to-improve-the-text-to-image-non-diffusion-prior/">MarkTechPost</a>, 
              <a href="https://multiplatform.ai/eclipse-a-game-changer-in-text-to-image-generation-unveiled-by-arizona-state-university/">MultiPlatformAI</a>, 
              <a href="https://m.youtube.com/watch?v=jjcMmIGottQ">Video discussion</a>, 
              <a href="https://paperbrief.net/posts/155/">Paper Digest</a>)
            </p>
            <p>
              Improving the Parameter and Data Efficiency of the Text-to-Image Priors for UnCLIP Family Models with contrastive loss.
            </p>
          </div>
        </div>

        <!-- Publication Item 5 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/WOUAF1.png" alt="WOUAF">
          </div>
          <div class="publication-content">
            <a href="https://wouaf.vercel.app/" class="papertitle">WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models</a>
            <p>
              <a href="https://www.changhoonkim.com/">Changhoon Kim</a>, 
              <a href="https://sites.google.com/view/kylemin">Kyle Min</a>, 
              <a href="https://maitreyapatel.com/">Maitreya Patel</a>, 
              <strong>Sheng Cheng</strong>, 
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
            </p>
            <p><em>CVPR 2024</em></p>
            <p>
              <a href="https://wouaf.vercel.app/">Project Page</a> / 
              <a href="https://huggingface.co/spaces/wouaf/WOUAF-Text-to-Image">Demo</a> / 
              <a href="https://arxiv.org/abs/2306.04744">arXiv</a>
            </p>
            <p>
              Enabling the Integration of up to 32-bit (~4 billion) fingerprints into Text-to-Image Diffusion Models without loss in image quality.
            </p>
          </div>
        </div>

        <!-- Publication Item 6 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/dy_obj.png" alt="Self-supervised Learning">
          </div>
          <div class="publication-content">
            <a href="https://openreview.net/forum?id=Y7et3Ow02l" class="papertitle">Self-supervised Learning to Discover Physical Objects and Predict Their Interactions from Raw Videos</a>
            <p>
              <strong>Sheng Cheng</strong>, 
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>, 
              <a href="https://sites.google.com/view/complexmateriallab/home?authuser=0">Yang Jiao</a>, 
              <a href="http://public.asu.edu/~yren32">Yi Ren</a>
            </p>
            <p><em>NeurIPS AI4Science workshop, 2023</em></p>
            <p>
              <a href="https://openreview.net/forum?id=Y7et3Ow02l">arXiv</a>
            </p>
            <p>
              Jointly learning to discover physical objects and predict their dynamics in the videos for physical environment.
            </p>
          </div>
        </div>

        <!-- Publication Item 7 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/ABA1.png" alt="Adversarial Bayesian Augmentation">
          </div>
          <div class="publication-content">
            <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Adversarial_Bayesian_Augmentation_for_Single-Source_Domain_Generalization_ICCV_2023_paper.pdf" class="papertitle">Adversarial Bayesian Augmentation for Single-Source Domain Generalization</a>
            <p>
              <strong>Sheng Cheng</strong>, 
              <a href="https://www.tejasgokhale.com/">Tejas Gokhale</a>, 
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
            </p>
            <p><em>ICCV 2023</em></p>
            <p>
              <a href="https://arxiv.org/abs/2307.09520">arXiv</a> / 
              <a href="https://github.com/shengcheng/ABA">Code</a>
            </p>
            <p>
              Adversarial Learning + Bayesian neural network for single-source domain generalization.
            </p>
          </div>
        </div>

        <!-- Publication Item 8 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/SSRGNN_gen.png" alt="SSR-GNNs">
          </div>
          <div class="publication-content">
            <a href="https://openaccess.thecvf.com/content/CVPR2022W/SketchDL/papers/Cheng_SSR-GNNs_Stroke-Based_Sketch_Representation_With_Graph_Neural_Networks_CVPRW_2022_paper.pdf" class="papertitle">SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks</a>
            <p>
              <strong>Sheng Cheng</strong>, 
              <a href="http://public.asu.edu/~yren32">Yi Ren</a>, 
              <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
            </p>
            <p><em>CVPR Sketch workshop, 2022</em></p>
            <p>
              <a href="https://arxiv.org/abs/2204.13153">arXiv</a> / 
              <a href="https://github.com/shengcheng/SSR-GNNs">Code</a>
            </p>
            <p>
              Transformation invariant sketch recognition by decomposing to strokes and composing by graph neural network.
            </p>
          </div>
        </div>

        <!-- Publication Item 9 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/material.jpeg" alt="Data-Driven Learning">
          </div>
          <div class="publication-content">
            <a href="https://www.sciencedirect.com/science/article/abs/pii/S1359645422001872" class="papertitle">Data-Driven Learning of Three-Point Correlation Functions as Microstructure Representations</a>
            <p>
              <strong>Sheng Cheng</strong>, 
              <a href="https://sites.google.com/view/complexmateriallab/home?authuser=0">Yang Jiao</a>, 
              <a href="http://public.asu.edu/~yren32">Yi Ren</a>
            </p>
            <p><em>Acta Materialia, 2022</em></p>
            <p>
              <a href="https://arxiv.org/abs/2109.02255">arXiv</a> / 
              <a href="https://github.com/shengcheng/Material_Reconstruction_Correlation">Code</a>
            </p>
            <p>
              Learning the microstructure representation by 3-point correlation functions.
            </p>
          </div>
        </div>

        <!-- Publication Item 10 -->
        <div class="publication">
          <div class="publication-image">
            <img src="images/robust_bayesian.png" alt="Evaluating Robustness">
          </div>
          <div class="publication-content">
            <a href="https://aisecure-workshop.github.io/amlcvpr2021/cr/12.pdf" class="papertitle">Evaluating the Robustness of Bayesian Neural Networks Against Different Types of Attacks</a>
            <p>
              <a href="https://www.linkedin.com/in/yutianpang/">Yutian Pang</a>, 
              <strong>Sheng Cheng</strong>, 
              <a href="https://www.linkedin.com/in/jueming-hu/">Jueming Hu</a>, 
              <a href="http://yongming.faculty.asu.edu/">Yongming Liu</a>
            </p>
            <p><em>CVPR Adversarial Machine Learning workshop, 2021</em></p>
            <p>
              <a href="https://arxiv.org/abs/2106.09223">arXiv</a>
            </p>
            <p>
              Evaluating the robustness gain of Bayesian neural networks on image classification tasks.
            </p>
          </div>
        </div>

      </section>

      <!-- Work Experience Section -->
      <section class="section work-experience">
        <h2>Work Experience</h2>
        <ul>
          <li>
            <strong>Bosch US</strong>, Summer 2024
            <ul>
              <li>Masked controlled autonomous driving video generation.</li>
            </ul>
          </li>
          <li>
            <strong>Amazon Alexa</strong>, Summer 2023
            <ul>
              <li>Zero-shot mask annotation free open-vocabulary semantic segmentation by the text-to-image model.</li>
            </ul>
          </li>
          <li>
            <strong>UltruFit.ai</strong>, Summer 2022
            <ul>
              <li>A real-time system evaluating and scoring the human exercises by cameras.</li>
            </ul>
          </li>
          <li>
            <strong>Hikvision Research</strong>, Spring 2019
            <ul>
              <li>Research on a new metric for super-resolution based on one-to-many mapping nature.</li>
            </ul>
          </li>
        </ul>
      </section>

      <!-- Service & Honors Section -->
      <section class="section service-honor">
        <h2>Service & Honors</h2>
        <ul>
          <li>Reviewer: CVPR, NeurIPS, TNNLS, TIP, ICLR</li>
          <li>2023-24 ASU Graduate College Travel Award</li>
          <li>CVPR 2024 Doctoral Consortium</li>
          <li>
            Organizer of ASU 
            <a href="https://frontier-topics-in-genai-seminar.vercel.app/">Frontier Topics in GenAI Seminar</a>
          </li>
        </ul>
      </section>

      <!-- Footer Section -->
      <footer class="footer">
        <p>
          This website template is taken from 
          <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
        </p>
      </footer>

    </div> <!-- End of Container -->
  </body>
</html>

<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Sheng Cheng</title>

    <meta name="author" content="Sheng Cheng">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/favicon/favicon.ico" type="image/x-icon">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">
    
  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:100%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Sheng Cheng
                </p>
                <p>I'm a PhD student at ASU. I'm working with <a href="https://yezhouyang.engineering.asu.edu/"> Yezhou Yang</a>, and co-advised by <a href="http://public.asu.edu/~yren32">Yi Ren</a>.
                  I closely collaborate with <a href="https://maitreyapatel.com/">Maitreya Patel</a>, <a href="https://www.changhoonkim.com/">Changhoon Kim</a>, <a href="https://sites.google.com/view/deqiankong/home">Deqian Kong</a>. 
                  Previously, I received my M.Eng. in Electrical Engineering from University of Illinois at Urbana-Champaign, working with <a href="https://ruoyus.github.io/">Ruoyu Sun</a>, and received B.S. from Huazhong University of Science and Technology.
                </p>
                <p style="text-align:center">
                  <a href="scheng53@asu.edu">Email</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?user=TWAwdYsAAAAJ&hl=en">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://twitter.com/chengshengcs">Twitter</a> &nbsp;/&nbsp;
                  <a href="https://github.com/shengcheng/">Github</a> &nbsp;/&nbsp;
                  <a href="https://www.linkedin.com/in/sheng-cheng-661826118/">Linkedin</a>
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:20px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  I'm interested in computer vision and machine learning. My focus lies in the areas of vision & language (particularly in Text-to-Image generation), domain generalization & robustness, and AI in Science.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    
            
    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/triplet.png' width="220">
        </div>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="">
          <span class="papertitle">TripletCLIP:  Improving Compositional Reasoning of CLIP via Vision-Language Negatives</span>
        </a>
        <br>
        <a href="https://maitreyapatel.com/">Maitreya Patel</a>,
        <a href="">Abhiram Kusumba</a>,
        <strong>Sheng Cheng</strong>,
        <a href="https://www.changhoonkim.com/">Changhoon Kim</a>,
        <a href="https://www.tejasgokhale.com/">Tejas Gokhale</a>,
        <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
      <br>
      <em>NeurIPS 2024</em>
        <br>
        <a href="">arXiv</a>(To appear)
        /
        <a href="">code</a>
        <p></p>
        <p>
          We enhance CLIP models by generating "hard" negative captions and images to improve their compositional reasoning ability. 
        </p>
      </td> 
    </tr>

            
    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/score.png' width="220">
        </div>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="">
          <span class="papertitle">Precision or Recall? An Analysis of Image Captions for Training Text-to-Image Generation Model</span>
        </a>
        <br>
        <strong>Sheng Cheng</strong>,
        <a href="https://maitreyapatel.com/">Maitreya Patel</a>,
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
      <br>
      <em>EMNLP 2024</em>, Findings
        <br>
        <a href="">arXiv</a>(To appear)
        /
        <a href="">code</a>
        <p></p>
        <p>
          We analyze the impact of precision and recall in human-annotated and synthetic captions on the training text-to-image models.
        </p>
      </td> 
    </tr>

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/ode.png' width="220">
        </div>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://www.arxiv.org/abs/2409.03845">
          <span class="papertitle">Latent Space Energy-based Neural ODEs</span>
        </a>
        <br>
        <strong>Sheng Cheng*</strong>,
        <a href="https://sites.google.com/view/deqiankong/home">Deqian Kong*</a>,
        <a href="http://www.stat.ucla.edu/~jxie/">Jianwen Xie</a>,
        <a href="https://klee44.github.io/">Kookjin Lee</a>,
        <a href="http://www.stat.ucla.edu/~ywu/">Ying Nian Wu‡</a>,
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang‡</a>
      <br>
      <em>preprint</em>, 2024
        <br>
        <a href="https://www.arxiv.org/abs/2409.03845">arXiv</a>
        <p></p>
        <p>
          Integrating energy-based prior model with Neural ODEs for latent space continuous-time sequence data modeling, training using MLE with MCMC instead of inference network.
        </p>
      </td> 
    </tr>
    
    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/eclipse_teaser.png' width="220">
        </div>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://eclipse-t2i.vercel.app/">
          <span class="papertitle">Revising Text-to-Image Prior for Improved Text Conditioned Image Generations</span>
        </a>
        <br>
        <a href="https://maitreyapatel.com/">Maitreya Patel</a>,
        <a href="https://www.changhoonkim.com/">Changhoon Kim</a>,
        <strong>Sheng Cheng</strong>,
        <a href="https://www.public.asu.edu/~cbaral/">Chitta Baral</a>,
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
      <br>
      <em>CVPR</em>, 2024
        <br>
        <a href="https://eclipse-t2i.vercel.app/">project page</a>
        /
        <a href="https://huggingface.co/spaces/ECLIPSE-Community/ECLIPSE-Kandinsky-v2.2">demo</a>
        /
        <a href="https://arxiv.org/abs/2312.04655">arXiv</a>
        /
        <a href="https://github.com/eclipse-t2i/eclipse-inference">code</a>
        / media coverage (<a href="https://x.com/_akhaliq/status/1734036192817971630?s=61&t=3KlNMV5Ioc9Fy-B0z6w57A">Twitter of AK</a>, <a href="https://www.marktechpost.com/2023/12/13/this-ai-research-from-arizona-state-university-unveil-eclipse-a-novel-contrastive-learning-strategy-to-improve-the-text-to-image-non-diffusion-prior/
        ">MarkTechPost</a>, <a href="https://multiplatform.ai/eclipse-a-game-changer-in-text-to-image-generation-unveiled-by-arizona-state-university/
        ">MultiPlatformAI</a>, <a href="https://m.youtube.com/watch?v=jjcMmIGottQ">Video discussion</a>, <a href="https://paperbrief.net/posts/155/
        ">Paper Digest</a>)
        <p></p>
        <p>
          Improving the Parameter and Data Efficiency of the Text-to-Image Priors for UnCLIP Family Models with contrastive loss.
        </p>
      </td> 
    </tr>

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/WOUAF1.png' width="180">
        </div>
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://wouaf.vercel.app/">
          <span class="papertitle">WOUAF: Weight Modulation for User Attribution and Fingerprinting in Text-to-Image Diffusion Models</span>
        </a>
        <br>
        <a href="https://www.changhoonkim.com/">Changhoon Kim</a>,
        <a href="https://sites.google.com/view/kylemin">Kyle Min</a>,
        <a href="https://maitreyapatel.com/">Maitreya Patel</a>,
        <strong>Sheng Cheng</strong>,
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
        <br>
        <em>CVPR</em>, 2024
        <br>
        <a href="https://wouaf.vercel.app/">project page</a>
        /
        <a href="https://huggingface.co/spaces/wouaf/WOUAF-Text-to-Image">demo</a>
        /
        <a href="https://arxiv.org/abs/2306.04744">arXiv</a>
        <p></p>
        <p>
          Enabling the Integration of up to 32-bit (~4 billion) fingerprints into Text-to-Image Diffusion Models without loss in image quality.        </p>
      </td>    
    </tr>

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/dy_obj.png' width="220">
        </div>
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://openreview.net/forum?id=Y7et3Ow02l">
          <span class="papertitle">Self-supervised Learning to Discover Physical Objects and Predict Their Interactions from Raw Videos</span>
        </a>
        <br>
        <strong>Sheng Cheng</strong>,
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>,
        <a href="https://sites.google.com/view/complexmateriallab/home?authuser=0">Yang Jiao</a>,
        <a href="http://public.asu.edu/~yren32">Yi Ren</a>
        <br>
        <em>NeurIPS AI4Science workshop</em>, 2023
        <br>
        <a href="https://openreview.net/forum?id=Y7et3Ow02l">arXiv</a>
        <p></p>
        <p>
          Jointly learning to discover physical objects and predict their dynamics in the videos for physical environment.      </p>
      </td> 
    </tr>
    
    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/ABA1.png' width="220">
        </div>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Cheng_Adversarial_Bayesian_Augmentation_for_Single-Source_Domain_Generalization_ICCV_2023_paper.pdf">
          <span class="papertitle">Adversarial Bayesian Augmentation for Single-Source Domain Generalization</span>
        </a>
        <br>
        <strong>Sheng Cheng</strong>,
        <a href="https://www.tejasgokhale.com/">Tejas Gokhale</a>,
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
        <br>
        <em>ICCV</em>, 2023
        <br>
        <a href="https://arxiv.org/abs/2307.09520">arXiv</a>
        /
        <a href="https://github.com/shengcheng/ABA">code</a>
        <p></p>
        <p>
            Adversarial Learning + Bayesian neural network for single-source domain generalization.
        </p>
      </td> 
    </tr>

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/SSRGNN_gen.png' width="220">
        </div>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://openaccess.thecvf.com/content/CVPR2022W/SketchDL/papers/Cheng_SSR-GNNs_Stroke-Based_Sketch_Representation_With_Graph_Neural_Networks_CVPRW_2022_paper.pdf">
          <span class="papertitle">SSR-GNNs: Stroke-based Sketch Representation with Graph Neural Networks</span>
        </a>
        <br>
        <strong>Sheng Cheng</strong>,
        <a href="http://public.asu.edu/~yren32">Yi Ren</a>,
        <a href="https://yezhouyang.engineering.asu.edu/">Yezhou Yang</a>
        <br>
        <em>CVPR Sketch workshop</em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2204.13153">arXiv</a>
        /
        <a href="https://github.com/shengcheng/SSR-GNNs">code</a>
        <p></p>
        <p>
          Transformation invariant sketch recognition by decomposing to strokes and composing by graph neural network. 
        </p>
      </td> 
    </tr>

    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/material.jpeg' width="220">
        </div>
        <script type="text/javascript">
          function camp_start() {
            document.getElementById('camp_image').style.opacity = "1";
          }

          function camp_stop() {
            document.getElementById('camp_image').style.opacity = "0";
          }
          camp_stop()
        </script>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://www.sciencedirect.com/science/article/abs/pii/S1359645422001872">
          <span class="papertitle">Data-Driven Learning of Three-Point Correlation Functions as Microstructure Representations</span>
        </a>
        <br>
        <strong>Sheng Cheng</strong>,
        <a href="https://sites.google.com/view/complexmateriallab/home?authuser=0">Yang Jiao</a>,
        <a href="http://public.asu.edu/~yren32">Yi Ren</a>
        <br>
        <em>Acta Materialia</em>, 2022
        <br>
        <a href="https://arxiv.org/abs/2109.02255">arXiv</a>
        /
        <a href="https://github.com/shengcheng/Material_Reconstruction_Correlation">code</a>
        <p></p>
        <p>
          Learning the microstructure representation by 3-point correlation functions.
        </p>
      </td> 
    </tr>
    
    <tr onmouseout="camp_stop()" onmouseover="camp_start()">
      <td style="padding:20px;width:30%;vertical-align:middle">
        <div class="one">
          <img src='images/robust_bayesian.png' width="220">
        </div>
      </td>
      <td style="padding:20px;width:65%;vertical-align:middle">
        <a href="https://aisecure-workshop.github.io/amlcvpr2021/cr/12.pdf">
          <span class="papertitle">Evaluating the Robustness of Bayesian Neural Networks Against Different Types of Attacks</span>
        </a>
        <br>
        <a href="https://www.linkedin.com/in/yutianpang/">Yutian Pang</a>,
        <strong>Sheng Cheng</strong>,
        <a href="https://www.linkedin.com/in/jueming-hu/">Jueming Hu</a>,
        <a href="http://yongming.faculty.asu.edu/">Yongming Liu</a>,
        <br>
        <em>CVPR Adversarial Machine Learning workshop</em>, 2021
        <br>
        <a href="https://arxiv.org/abs/2106.09223">arXiv</a>
        <p></p>
        <p>
          Evaluating the robustness gain of Bayesian neural networks on image classification tasks.
        </p>
      </td> 
    </tr>
            
    

    
 

          
          <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
            <tr>
              <td>
                <h2>Work Experience</h2>
              </td>
            </tr>
          </tbody></table>
          <table width="100%" align="center" border="0" cellpadding="20"><tbody>
            
            <tr>
              <td width="100%" valign="center">
                <br>
                Bosch US, 2024 Summer
                <br>

                Amazon Alexa, 2023 Summer
                <br>
                <li>Zero-shot mask annotation free open-vocabulary semantic segmentation by the text-to-image model.
                </li>
                <br>
                UltruFit.ai, 2022 Summer
                
                <li> a real-time system evaluating and scoring the human exercises by cameras.
                </li>
                <br>
                Hikvision Research, 2019 Spring
                <br>
                <li>
                Research on a new metric for super-resolution based on one-to-many mapping nature.
                </li>
              </td>
            </tr>
            <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20"><tbody>
              <tr>
                <td>
                  <h2>Service & Honor</h2>
                  Reviewer: CVPR, Neurips, TNNLS, TIP
                  <br>
                  2023-24 ASU Graduate College Travel Award
                  <br>
                  CVPR 2024 Doctoral Consortium 
                  <br>
                  Organizer of ASU <a href="https://frontier-topics-in-genai-seminar.vercel.app/">Frontier Topics in GenAI Seminar</a>
                </td>
              </tr>
            </tbody></table>
            <table width="100%" align="center" border="0" cellpadding="20"><tbody>
 

            
            
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:left;font-size:small;">
                  This website template is taken from <a href="https://github.com/jonbarron/jonbarron_website">source code</a>.
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
